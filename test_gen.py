from faster_whisper import WhisperModel
from opencc import OpenCC
from tqdm import tqdm
import torch

# è¨­å®šæ¨¡å‹èˆ‡æª”æ¡ˆè·¯å¾‘
device = torch.device('cpu')
if torch.cuda.is_available():
    print('cuda available!')
    device = torch.device('cuda')
else:
    print('cuda unvailable')

model = WhisperModel("small", device="cuda",device_index=0,compute_type="float16")  # ä½ å¯æ›æˆ tiny æˆ– medium

cc = OpenCC('s2t')

# ğŸ’¾ å¯«æ­»çš„æª”æ¡ˆè·¯å¾‘
input_audio = "å°æ˜Ÿæ˜Ÿ (Twinkle Twinkle Little Star) Chinese with Pinyin Subtitles Sing Along_Vocals.wav"
output_srt = "å°æ˜Ÿæ˜Ÿ (Twinkle Twinkle Little Star) Chinese with Pinyin Subtitles Sing Along_Vocals.srt"

# åŸ·è¡Œè½‰éŒ„
print("ğŸ™ï¸ é–‹å§‹è½‰éŒ„éŸ³è¨Šï¼ˆå«é€å­—æ™‚é–“ï¼‰...")
segments, _ = model.transcribe(input_audio, word_timestamps=True)

# æ ¼å¼åŒ–æ™‚é–“
def format_timestamp(seconds):
    hrs = int(seconds // 3600)
    mins = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hrs:02}:{mins:02}:{secs:02},{millis:03}"

# å¯«å…¥ .srt
with open(output_srt, "w", encoding="utf-8") as f:
    for i, segment in enumerate(tqdm(segments, desc="ğŸ“ ç”Ÿæˆé€å­— SRT")):
        words = segment.words
        if not words:
            continue
        start = format_timestamp(words[0].start)  # ğŸ¯ ä»¥ç¬¬ä¸€å€‹å­—çš„æ™‚é–“ç‚ºèµ·é»
        end = format_timestamp(words[-1].end)
        text = cc.convert("".join([w.word for w in words]).strip())
        f.write(f"{i+1}\n{start} --> {end}\n{text}\n\n")

print(f"âœ… ç²¾ç¢ºé€å­—å­—å¹•å„²å­˜è‡³ï¼š{output_srt}")

